"""
test_ssot_job.py - SSOT (job.json) ê´€ë¦¬ í…ŒìŠ¤íŠ¸

DoD:
1. job.json SSOT + lock + mismatch/corrupt reject
"""

import json
import os
import threading
import time
from datetime import UTC, datetime
from pathlib import Path
from unittest.mock import patch

import pytest

from src.core.ssot_job import (
    atomic_write_json,
    atomic_write_json_exclusive,
    ensure_job_json,
    job_lock,
    load_job_json,
    verify_mismatch,
)
from src.domain.errors import ErrorCodes, PolicyRejectError

# =============================================================================
# job_lock í…ŒìŠ¤íŠ¸
# =============================================================================


class TestJobLock:
    """job_lock ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸."""

    def test_lock_acquired_and_released(self, tmp_path: Path, test_config: dict):
        """ë½ íšë“ í›„ ì •ìƒ í•´ì œ í™•ì¸."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        lock_dir = job_dir / ".lock"

        # ë½ íšë“ ì „
        assert not lock_dir.exists()

        # ë½ íšë“ ì¤‘
        with job_lock(job_dir, test_config) as acquired_lock:
            assert lock_dir.exists()
            assert acquired_lock == lock_dir

        # ë½ í•´ì œ í›„
        assert not lock_dir.exists()

    def test_lock_released_on_exception(self, tmp_path: Path, test_config: dict):
        """ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë½ í•´ì œ í™•ì¸."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        lock_dir = job_dir / ".lock"

        with pytest.raises(ValueError, match="test error"):
            with job_lock(job_dir, test_config):
                assert lock_dir.exists()
                raise ValueError("test error")

        # ì˜ˆì™¸ í›„ì—ë„ ë½ í•´ì œë¨
        assert not lock_dir.exists()

    def test_lock_timeout_raises_error(self, tmp_path: Path):
        """ë½ íƒ€ì„ì•„ì›ƒ ì‹œ PolicyRejectError ë°œìƒ."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        # ì‚¬ì „ì— ë½ í´ë” ìƒì„± (ì ê¸ˆ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)
        lock_dir = job_dir / ".lock"
        lock_dir.mkdir()

        # ì§§ì€ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        config = {
            "paths": {"lock_dir": ".lock"},
            "pipeline": {
                "lock_retry_interval": 0.01,
                "lock_max_retries": 2,
            },
        }

        with pytest.raises(PolicyRejectError) as exc_info:
            with job_lock(job_dir, config):
                pass

        assert exc_info.value.code == ErrorCodes.JOB_JSON_LOCK_TIMEOUT
        assert "job_dir" in exc_info.value.context

    def test_concurrent_lock_acquisition(self, tmp_path: Path, test_config: dict):
        """ë™ì‹œ ë½ íšë“ ì‹œ í•œ ìª½ë§Œ ì„±ê³µ."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        results = []
        barrier = threading.Barrier(2)

        def try_lock(name: str):
            barrier.wait()  # ë™ì‹œ ì‹œì‘
            try:
                with job_lock(job_dir, test_config):
                    time.sleep(0.1)  # ë½ ë³´ìœ 
                    results.append(f"{name}:success")
            except PolicyRejectError:
                results.append(f"{name}:timeout")

        t1 = threading.Thread(target=try_lock, args=("t1",))
        t2 = threading.Thread(target=try_lock, args=("t2",))

        t1.start()
        t2.start()
        t1.join()
        t2.join()

        # ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” ì„±ê³µ, í•˜ë‚˜ëŠ” timeout
        success_count = sum(1 for r in results if "success" in r)

        assert success_count >= 1  # ìµœì†Œ 1ê°œ ì„±ê³µ
        # Note: íƒ€ì´ë°ì— ë”°ë¼ ë‘˜ ë‹¤ ì„±ê³µí•  ìˆ˜ ìˆìŒ (ìˆœì°¨ì  íšë“)


# =============================================================================
# atomic_write_json í…ŒìŠ¤íŠ¸
# =============================================================================


class TestAtomicWriteJson:
    """atomic_write_json í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""

    def test_writes_json_correctly(self, tmp_path: Path):
        """JSON íŒŒì¼ ì •ìƒ ì‘ì„±."""
        file_path = tmp_path / "test.json"
        data = {"key": "value", "number": 42, "í•œê¸€": "í…ŒìŠ¤íŠ¸"}

        atomic_write_json(file_path, data)

        assert file_path.exists()
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        assert loaded == data

    def test_creates_parent_directories(self, tmp_path: Path):
        """ë¶€ëª¨ ë””ë ‰í„°ë¦¬ ìë™ ìƒì„±."""
        file_path = tmp_path / "nested" / "dir" / "test.json"
        data = {"key": "value"}

        atomic_write_json(file_path, data)

        assert file_path.exists()
        assert file_path.parent.exists()

    def test_overwrites_existing_file(self, tmp_path: Path):
        """ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°."""
        file_path = tmp_path / "test.json"
        file_path.write_text('{"old": "data"}')

        new_data = {"new": "data"}
        atomic_write_json(file_path, new_data)

        loaded = json.loads(file_path.read_text())
        assert loaded == new_data

    def test_no_temp_file_left_on_success(self, tmp_path: Path):
        """ì„±ê³µ ì‹œ temp íŒŒì¼ ë‚¨ì§€ ì•ŠìŒ."""
        file_path = tmp_path / "test.json"
        data = {"key": "value"}

        atomic_write_json(file_path, data)

        # .tmp íŒŒì¼ ì—†ì–´ì•¼ í•¨
        tmp_files = list(tmp_path.glob("*.tmp"))
        assert len(tmp_files) == 0

    def test_preserves_original_on_failure(self, tmp_path: Path):
        """ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ë³´ì¡´ (ì“°ê¸° ì—ëŸ¬ ì‹œë®¬ë ˆì´ì…˜)."""
        file_path = tmp_path / "test.json"
        original_data = {"original": "data"}
        file_path.write_text(json.dumps(original_data))

        # json.dumpê°€ ì‹¤íŒ¨í•˜ë„ë¡ ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´ ì „ë‹¬
        class NonSerializable:
            pass

        with pytest.raises(TypeError):
            atomic_write_json(file_path, {"bad": NonSerializable()})

        # ì›ë³¸ íŒŒì¼ ìœ ì§€ë¨
        loaded = json.loads(file_path.read_text())
        assert loaded == original_data


# =============================================================================
# verify_mismatch í…ŒìŠ¤íŠ¸
# =============================================================================


class TestVerifyMismatch:
    """verify_mismatch í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""

    def test_no_error_when_match(self):
        """wo_no, lineì´ ì¼ì¹˜í•˜ë©´ ì—ëŸ¬ ì—†ìŒ."""
        existing = {"wo_no": "WO-001", "line": "L1"}
        current = {"wo_no": "WO-001", "line": "L1", "extra": "ignored"}

        # ì—ëŸ¬ ì—†ì´ í†µê³¼
        verify_mismatch(existing, current)

    def test_raises_on_wo_no_mismatch(self):
        """wo_no ë¶ˆì¼ì¹˜ ì‹œ PolicyRejectError ë°œìƒ."""
        existing = {"wo_no": "WO-001", "line": "L1"}
        current = {"wo_no": "WO-002", "line": "L1"}

        with pytest.raises(PolicyRejectError) as exc_info:
            verify_mismatch(existing, current)

        assert exc_info.value.code == ErrorCodes.PACKET_JOB_MISMATCH
        assert exc_info.value.context["field"] == "wo_no"
        assert exc_info.value.context["existing"] == "WO-001"
        assert exc_info.value.context["current"] == "WO-002"

    def test_raises_on_line_mismatch(self):
        """line ë¶ˆì¼ì¹˜ ì‹œ PolicyRejectError ë°œìƒ."""
        existing = {"wo_no": "WO-001", "line": "L1"}
        current = {"wo_no": "WO-001", "line": "L2"}

        with pytest.raises(PolicyRejectError) as exc_info:
            verify_mismatch(existing, current)

        assert exc_info.value.code == ErrorCodes.PACKET_JOB_MISMATCH
        assert exc_info.value.context["field"] == "line"


# =============================================================================
# load_job_json í…ŒìŠ¤íŠ¸
# =============================================================================


class TestLoadJobJson:
    """load_job_json í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""

    def test_loads_valid_json(self, tmp_path: Path):
        """ì •ìƒ JSON íŒŒì¼ ë¡œë“œ."""
        job_json_path = tmp_path / "job.json"
        data = {"job_id": "JOB-001", "wo_no": "WO-001", "line": "L1"}
        job_json_path.write_text(json.dumps(data))

        loaded = load_job_json(job_json_path)

        assert loaded == data

    def test_raises_on_corrupt_json(self, tmp_path: Path):
        """ì†ìƒëœ JSON ì‹œ PolicyRejectError ë°œìƒ."""
        job_json_path = tmp_path / "job.json"
        job_json_path.write_text("{ invalid json }")

        with pytest.raises(PolicyRejectError) as exc_info:
            load_job_json(job_json_path)

        assert exc_info.value.code == ErrorCodes.JOB_JSON_CORRUPT
        assert "path" in exc_info.value.context


# =============================================================================
# ensure_job_json í…ŒìŠ¤íŠ¸
# =============================================================================


class TestEnsureJobJson:
    """ensure_job_json í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""

    def test_creates_new_job_json(self, tmp_path: Path, test_config: dict):
        """job.json ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        packet = {"wo_no": "WO-001", "line": "L1"}

        def generate_id(p):
            return f"JOB-{p['wo_no']}-{p['line']}"

        job_id = ensure_job_json(job_dir, packet, test_config, generate_id)

        assert job_id == "JOB-WO-001-L1"

        # job.json ìƒì„± í™•ì¸
        job_json_path = job_dir / "job.json"
        assert job_json_path.exists()

        job_data = json.loads(job_json_path.read_text())
        assert job_data["job_id"] == "JOB-WO-001-L1"
        assert job_data["wo_no"] == "WO-001"
        assert job_data["line"] == "L1"
        assert "created_at" in job_data
        assert job_data["schema_version"] == "1.0"

    def test_returns_existing_job_id(self, tmp_path: Path, test_config: dict):
        """ê¸°ì¡´ job.json ìˆìœ¼ë©´ job_id ë°˜í™˜."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        # ê¸°ì¡´ job.json ìƒì„±
        existing_data = {
            "job_id": "EXISTING-JOB-ID",
            "wo_no": "WO-001",
            "line": "L1",
        }
        (job_dir / "job.json").write_text(json.dumps(existing_data))

        packet = {"wo_no": "WO-001", "line": "L1"}

        def generate_id(p):  # noqa: ARG001
            return "NEW-JOB-ID"  # í˜¸ì¶œë˜ì§€ ì•Šì•„ì•¼ í•¨

        job_id = ensure_job_json(job_dir, packet, test_config, generate_id)

        assert job_id == "EXISTING-JOB-ID"  # ê¸°ì¡´ ID ë°˜í™˜

    def test_raises_on_mismatch(self, tmp_path: Path, test_config: dict):
        """wo_no/line ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬."""
        job_dir = tmp_path / "job"
        job_dir.mkdir()

        # ê¸°ì¡´ job.json
        existing_data = {"job_id": "JOB-001", "wo_no": "WO-001", "line": "L1"}
        (job_dir / "job.json").write_text(json.dumps(existing_data))

        # ë‹¤ë¥¸ wo_noë¡œ ì‹œë„
        packet = {"wo_no": "WO-DIFFERENT", "line": "L1"}

        with pytest.raises(PolicyRejectError) as exc_info:
            ensure_job_json(job_dir, packet, test_config, lambda p: "NEW")

        assert exc_info.value.code == ErrorCodes.PACKET_JOB_MISMATCH


# =============================================================================
# Lock í•´ì œ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸
# =============================================================================


class TestJobLockReleaseFailure:
    """ë½ í•´ì œ ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸ í…ŒìŠ¤íŠ¸."""

    def test_lock_release_failure_logs_warning(
        self, tmp_path: Path, test_config: dict, caplog
    ):
        """ë½ í•´ì œ ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸ê°€ ë‚¨ëŠ”ì§€ í™•ì¸."""
        import logging

        from src.core import ssot_job

        job_dir = tmp_path / "job"
        job_dir.mkdir()

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        # ì›ë³¸ os.rmdir ì €ì¥
        original_rmdir = os.rmdir

        def failing_rmdir(path):
            raise OSError("Permission denied")

        # job_lock ì§„ì… í›„ rmdirë§Œ mock
        with job_lock(job_dir, test_config):
            # ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ rmdirì„ ì‹¤íŒ¨í•˜ë„ë¡ êµì²´
            ssot_job.os.rmdir = failing_rmdir

        # ì›ë³¸ ë³µì›
        ssot_job.os.rmdir = original_rmdir

        # warning ë¡œê·¸ í™•ì¸
        assert any("Lock release failed" in record.message for record in caplog.records)
        assert any(
            "Manual cleanup may be required" in record.message
            for record in caplog.records
        )

    def test_lock_release_failure_includes_path_in_log(
        self, tmp_path: Path, test_config: dict, caplog
    ):
        """ë½ í•´ì œ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ì— ê²½ë¡œ ì •ë³´ í¬í•¨ í™•ì¸."""
        import logging

        job_dir = tmp_path / "job"
        job_dir.mkdir()

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        with patch("src.core.ssot_job.os.rmdir") as mock_rmdir:
            mock_rmdir.side_effect = OSError("Device busy")

            with job_lock(job_dir, test_config):
                pass

        # ë¡œê·¸ì— job_dir ê²½ë¡œ í¬í•¨
        warning_logs = [r.message for r in caplog.records if r.levelname == "WARNING"]
        assert len(warning_logs) >= 1
        assert str(job_dir) in warning_logs[0]


# =============================================================================
# fsync ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸
# =============================================================================


class TestAtomicWriteFsyncFailure:
    """fsync ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸ í…ŒìŠ¤íŠ¸."""

    def test_fsync_failure_logs_warning(self, tmp_path: Path, caplog):
        """fsync ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸ê°€ ë‚¨ëŠ”ì§€ í™•ì¸."""
        import logging

        file_path = tmp_path / "test.json"
        data = {"key": "value"}

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        with patch("os.fsync") as mock_fsync:
            mock_fsync.side_effect = OSError("I/O error")

            atomic_write_json(file_path, data)

        # warning ë¡œê·¸ í™•ì¸
        assert any("fsync failed" in record.message for record in caplog.records)
        assert any(
            "Data may not be durable" in record.message for record in caplog.records
        )

    def test_fsync_failure_still_writes_file(self, tmp_path: Path):
        """fsync ì‹¤íŒ¨í•´ë„ íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì‘ì„±ë¨."""
        file_path = tmp_path / "test.json"
        data = {"key": "value", "number": 42}

        with patch("os.fsync") as mock_fsync:
            mock_fsync.side_effect = OSError("I/O error")

            atomic_write_json(file_path, data)

        # íŒŒì¼ì€ ì •ìƒ ì‘ì„±ë¨
        assert file_path.exists()
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        assert loaded == data

    def test_fsync_failure_includes_path_in_log(self, tmp_path: Path, caplog):
        """fsync ì‹¤íŒ¨ ì‹œ ë¡œê·¸ì— íŒŒì¼ ê²½ë¡œ í¬í•¨ í™•ì¸."""
        import logging

        file_path = tmp_path / "important.json"
        data = {"key": "value"}

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        with patch("os.fsync") as mock_fsync:
            mock_fsync.side_effect = OSError("Disk full")

            atomic_write_json(file_path, data)

        # ë¡œê·¸ì— íŒŒì¼ ê²½ë¡œ í¬í•¨
        warning_logs = [r.message for r in caplog.records if r.levelname == "WARNING"]
        assert len(warning_logs) >= 1
        assert str(file_path) in warning_logs[0]

    def test_fsync_success_no_warning(self, tmp_path: Path, caplog):
        """fsync ì„±ê³µ ì‹œ warning ì—†ìŒ."""
        import logging

        file_path = tmp_path / "test.json"
        data = {"key": "value"}

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        # fsync ì •ìƒ ë™ì‘ (mock ì—†ì´)
        atomic_write_json(file_path, data)

        # fsync ê´€ë ¨ warning ì—†ìŒ
        fsync_warnings = [r for r in caplog.records if "fsync" in r.message]
        assert len(fsync_warnings) == 0


# =============================================================================
# ë””ë ‰í† ë¦¬ fsync í…ŒìŠ¤íŠ¸
# =============================================================================


class TestDirectoryFsync:
    """ë””ë ‰í† ë¦¬ fsync í…ŒìŠ¤íŠ¸."""

    def test_dir_fsync_failure_logs_warning(self, tmp_path: Path, caplog):
        """ë””ë ‰í† ë¦¬ fsync ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸ê°€ ë‚¨ëŠ”ì§€ í™•ì¸."""
        import logging

        from src.core.ssot_job import _fsync_dir

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        with patch("os.open") as mock_open:
            mock_open.side_effect = OSError("Operation not permitted")

            _fsync_dir(tmp_path)

        # warning ë¡œê·¸ í™•ì¸
        assert any(
            "Directory fsync failed" in record.message for record in caplog.records
        )

    def test_dir_fsync_success_no_warning(self, tmp_path: Path, caplog):
        """ë””ë ‰í† ë¦¬ fsync ì„±ê³µ ì‹œ warning ì—†ìŒ."""
        import logging

        from src.core.ssot_job import _fsync_dir

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        # ì‹¤ì œ ë””ë ‰í† ë¦¬ fsync (Linuxì—ì„œëŠ” ì„±ê³µí•´ì•¼ í•¨)
        _fsync_dir(tmp_path)

        # dir fsync ê´€ë ¨ warning ì—†ìŒ
        dir_fsync_warnings = [
            r for r in caplog.records if "Directory fsync" in r.message
        ]
        assert len(dir_fsync_warnings) == 0

    def test_atomic_write_calls_dir_fsync(self, tmp_path: Path):
        """atomic_write_jsonì´ ë””ë ‰í† ë¦¬ fsyncë¥¼ í˜¸ì¶œí•˜ëŠ”ì§€ í™•ì¸."""
        from src.core import ssot_job

        file_path = tmp_path / "test.json"
        data = {"key": "value"}

        dir_fsync_called = []
        original_fsync_dir = ssot_job._fsync_dir

        def tracking_fsync_dir(dir_path):
            dir_fsync_called.append(dir_path)
            return original_fsync_dir(dir_path)

        ssot_job._fsync_dir = tracking_fsync_dir
        try:
            atomic_write_json(file_path, data)
        finally:
            ssot_job._fsync_dir = original_fsync_dir

        # ë””ë ‰í† ë¦¬ fsyncê°€ í˜¸ì¶œë¨
        assert len(dir_fsync_called) == 1
        assert dir_fsync_called[0] == tmp_path


# =============================================================================
# Stale Lock ë©”íƒ€ì •ë³´ í…ŒìŠ¤íŠ¸
# =============================================================================


class TestStaleLockMeta:
    """Stale lock ë©”íƒ€ì •ë³´ í…ŒìŠ¤íŠ¸."""

    def test_lock_creates_meta_file(self, tmp_path: Path, test_config: dict):
        """ë½ íšë“ ì‹œ ë©”íƒ€ íŒŒì¼ì´ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸."""
        from src.core.ssot_job import LOCK_META_FILENAME

        job_dir = tmp_path / "job"
        job_dir.mkdir()

        with job_lock(job_dir, test_config) as lock_dir:
            meta_path = lock_dir / LOCK_META_FILENAME
            assert meta_path.exists()

            # ë©”íƒ€ ë‚´ìš© í™•ì¸
            meta = json.loads(meta_path.read_text())
            assert "pid" in meta
            assert "hostname" in meta
            assert "created_at" in meta
            assert meta["pid"] == os.getpid()

    def test_lock_removes_meta_file_on_release(self, tmp_path: Path, test_config: dict):
        """ë½ í•´ì œ ì‹œ ë©”íƒ€ íŒŒì¼ì´ ì‚­ì œë˜ëŠ”ì§€ í™•ì¸."""
        from src.core.ssot_job import LOCK_META_FILENAME

        job_dir = tmp_path / "job"
        job_dir.mkdir()
        lock_dir = job_dir / ".lock"

        with job_lock(job_dir, test_config):
            pass

        # ë½ í•´ì œ í›„ ë©”íƒ€ íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ëª¨ë‘ ì—†ì–´ì•¼ í•¨
        assert not lock_dir.exists()
        assert not (lock_dir / LOCK_META_FILENAME).exists()

    def test_stale_lock_detected_by_dead_pid(self, tmp_path: Path):
        """ì£½ì€ PIDì˜ ë½ì€ staleë¡œ ê°ì§€ë˜ëŠ”ì§€ í™•ì¸."""
        from src.core.ssot_job import (
            LOCK_META_FILENAME,
            _get_current_hostname,
            _is_stale_lock,
        )

        lock_dir = tmp_path / ".lock"
        lock_dir.mkdir()

        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” PIDë¡œ ë©”íƒ€ ì‘ì„±
        meta = {
            "pid": 999999999,  # ì¡´ì¬í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„± ë†’ì€ PID
            "hostname": _get_current_hostname(),
            "created_at": datetime.now(UTC).isoformat(),
        }
        (lock_dir / LOCK_META_FILENAME).write_text(json.dumps(meta))

        # staleë¡œ ê°ì§€ë˜ì–´ì•¼ í•¨ (PIDê°€ ì£½ì—ˆìœ¼ë¯€ë¡œ)
        assert _is_stale_lock(lock_dir) is True

    def test_stale_lock_not_detected_for_alive_pid(self, tmp_path: Path):
        """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ PIDì˜ ë½ì€ staleì´ ì•„ë‹˜."""
        from src.core.ssot_job import (
            LOCK_META_FILENAME,
            _get_current_hostname,
            _is_stale_lock,
        )

        lock_dir = tmp_path / ".lock"
        lock_dir.mkdir()

        # í˜„ì¬ PIDë¡œ ë©”íƒ€ ì‘ì„±
        meta = {
            "pid": os.getpid(),
            "hostname": _get_current_hostname(),
            "created_at": datetime.now(UTC).isoformat(),
        }
        (lock_dir / LOCK_META_FILENAME).write_text(json.dumps(meta))

        # staleì´ ì•„ë‹ˆì–´ì•¼ í•¨ (í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‚´ì•„ìˆìœ¼ë¯€ë¡œ)
        assert _is_stale_lock(lock_dir) is False

    def test_stale_lock_ttl_for_different_host(self, tmp_path: Path):
        """ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì˜ ë½ì€ TTL ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨."""
        from src.core.ssot_job import (
            LOCK_META_FILENAME,
            STALE_LOCK_THRESHOLD_SECONDS,
            _is_stale_lock,
        )

        lock_dir = tmp_path / ".lock"
        lock_dir.mkdir()

        # ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸, ìµœê·¼ ì‹œê°„
        meta = {
            "pid": 12345,
            "hostname": "other-host-that-does-not-exist",
            "created_at": datetime.now(UTC).isoformat(),
        }
        (lock_dir / LOCK_META_FILENAME).write_text(json.dumps(meta))

        # ìµœê·¼ì´ë¯€ë¡œ stale ì•„ë‹˜
        assert _is_stale_lock(lock_dir) is False

        # ì˜¤ë˜ëœ ì‹œê°„ìœ¼ë¡œ ë³€ê²½
        from datetime import timedelta

        old_time = datetime.now(UTC) - timedelta(
            seconds=STALE_LOCK_THRESHOLD_SECONDS + 100
        )
        meta["created_at"] = old_time.isoformat()
        (lock_dir / LOCK_META_FILENAME).write_text(json.dumps(meta))

        # TTL ì´ˆê³¼ë¡œ stale
        assert _is_stale_lock(lock_dir) is True

    def test_cleanup_stale_lock_logs_owner_info(self, tmp_path: Path, caplog):
        """stale lock ì •ë¦¬ ì‹œ ì†Œìœ ì ì •ë³´ê°€ ë¡œê·¸ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸."""
        import logging
        from datetime import timedelta

        from src.core.ssot_job import (
            LOCK_META_FILENAME,
            STALE_LOCK_THRESHOLD_SECONDS,
            _try_cleanup_stale_lock,
        )

        lock_dir = tmp_path / ".lock"
        lock_dir.mkdir()

        # ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì˜ ì˜¤ë˜ëœ ë½ (TTL ì´ˆê³¼)
        old_time = datetime.now(UTC) - timedelta(
            seconds=STALE_LOCK_THRESHOLD_SECONDS + 100
        )
        meta = {
            "pid": 999999999,
            "hostname": "test-host",
            "created_at": old_time.isoformat(),
        }
        (lock_dir / LOCK_META_FILENAME).write_text(json.dumps(meta))

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        result = _try_cleanup_stale_lock(lock_dir)

        assert result is True
        assert not lock_dir.exists()

        # ë¡œê·¸ì— ì†Œìœ ì ì •ë³´ í¬í•¨
        warning_logs = [r.message for r in caplog.records if r.levelname == "WARNING"]
        assert len(warning_logs) >= 1
        assert "pid=999999999" in warning_logs[0]
        assert "host=test-host" in warning_logs[0]


# =============================================================================
# atomic_write_json_exclusive í…ŒìŠ¤íŠ¸ (O_EXCL íŒ¨í„´)
# =============================================================================


class TestAtomicWriteJsonExclusive:
    """atomic_write_json_exclusive í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (TOCTOU-safe)."""

    def test_creates_new_file_returns_true(self, tmp_path: Path):
        """ìƒˆ íŒŒì¼ ìƒì„± ì‹œ True ë°˜í™˜."""
        file_path = tmp_path / "new.json"
        data = {"key": "value", "number": 42}

        result = atomic_write_json_exclusive(file_path, data)

        assert result is True
        assert file_path.exists()
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        assert loaded == data

    def test_existing_file_returns_false(self, tmp_path: Path):
        """ê¸°ì¡´ íŒŒì¼ ì¡´ì¬ ì‹œ False ë°˜í™˜ ë° ë®ì–´ì“°ì§€ ì•ŠìŒ."""
        file_path = tmp_path / "existing.json"
        original_data = {"original": "data"}
        file_path.write_text(json.dumps(original_data))

        new_data = {"new": "data"}
        result = atomic_write_json_exclusive(file_path, new_data)

        assert result is False
        # ì›ë³¸ ë°ì´í„° ìœ ì§€ë¨
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        assert loaded == original_data

    def test_creates_parent_directories(self, tmp_path: Path):
        """ë¶€ëª¨ ë””ë ‰í„°ë¦¬ ìë™ ìƒì„±."""
        file_path = tmp_path / "nested" / "deep" / "dir" / "file.json"
        data = {"nested": True}

        result = atomic_write_json_exclusive(file_path, data)

        assert result is True
        assert file_path.exists()
        assert file_path.parent.exists()

    def test_unicode_content(self, tmp_path: Path):
        """ìœ ë‹ˆì½”ë“œ ë‚´ìš© ì •ìƒ ì²˜ë¦¬."""
        file_path = tmp_path / "unicode.json"
        data = {"í•œê¸€": "í…ŒìŠ¤íŠ¸", "emoji": "ğŸ‰", "japanese": "æ—¥æœ¬èª"}

        result = atomic_write_json_exclusive(file_path, data)

        assert result is True
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        assert loaded == data

    def test_concurrent_writes_only_one_wins(self, tmp_path: Path):
        """ë™ì‹œ ì“°ê¸° ì‹œ í•˜ë‚˜ë§Œ ì„±ê³µ (TOCTOU-safe ê²€ì¦)."""
        file_path = tmp_path / "race.json"
        results = []
        barrier = threading.Barrier(2)

        def try_write(name: str, data: dict):
            barrier.wait()  # ë™ì‹œ ì‹œì‘
            result = atomic_write_json_exclusive(file_path, data)
            results.append((name, result))

        t1 = threading.Thread(target=try_write, args=("t1", {"writer": "t1", "id": 1}))
        t2 = threading.Thread(target=try_write, args=("t2", {"writer": "t2", "id": 2}))

        t1.start()
        t2.start()
        t1.join()
        t2.join()

        # ì •í™•íˆ í•˜ë‚˜ë§Œ True (ì„±ê³µ)
        success_count = sum(1 for _, r in results if r is True)
        failure_count = sum(1 for _, r in results if r is False)

        assert success_count == 1
        assert failure_count == 1

        # íŒŒì¼ ë‚´ìš© í™•ì¸ - ìŠ¹ìì˜ ë°ì´í„°ê°€ ìˆì–´ì•¼ í•¨
        loaded = json.loads(file_path.read_text(encoding="utf-8"))
        winner = next(name for name, r in results if r is True)
        assert loaded["writer"] == winner

    def test_write_failure_cleans_up(self, tmp_path: Path):
        """ì“°ê¸° ì‹¤íŒ¨ ì‹œ ë¶ˆì™„ì „í•œ íŒŒì¼ ì •ë¦¬."""
        file_path = tmp_path / "fail.json"

        # ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´
        class NonSerializable:
            pass

        with pytest.raises(TypeError):
            atomic_write_json_exclusive(file_path, {"bad": NonSerializable()})

        # ë¶ˆì™„ì „í•œ íŒŒì¼ì´ ë‚¨ì•„ìˆì§€ ì•Šì•„ì•¼ í•¨
        assert not file_path.exists()

    def test_fsync_failure_logs_warning(self, tmp_path: Path, caplog):
        """fsync ì‹¤íŒ¨ ì‹œ warning ë¡œê·¸."""
        import logging

        file_path = tmp_path / "fsync_warn.json"
        data = {"key": "value"}

        caplog.set_level(logging.WARNING, logger="src.core.ssot_job")

        with patch("os.fsync") as mock_fsync:
            mock_fsync.side_effect = OSError("I/O error")
            result = atomic_write_json_exclusive(file_path, data)

        assert result is True
        # íŒŒì¼ì€ ì •ìƒ ìƒì„±ë¨
        assert file_path.exists()
        # warning ë¡œê·¸ í™•ì¸
        assert any("fsync failed" in record.message for record in caplog.records)
